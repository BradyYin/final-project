from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import torch

# âœ… æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name)

# âœ… åŠ è½½èŠå¤©æ–‡æœ¬æ•°æ®
with open("my_data_500_deduplicated.txt", "r", encoding="utf-8") as f:
    blocks = f.read().strip().split("\n\n")

# âœ… æ„å»ºæ•°æ®é›†
data = [{"text": block.replace("\n", " ").strip()} for block in blocks if len(block.strip()) > 10]
dataset = Dataset.from_list(data)

def tokenize(example):
    result = tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=128,
    )
    result["labels"] = result["input_ids"].copy()
    return result


tokenized_dataset = dataset.map(tokenize, batched=True)

training_args = TrainingArguments(
    output_dir="./chat-model",
    overwrite_output_dir=True,
    num_train_epochs=10,              # å¤šè½®è®­ç»ƒ
    per_device_train_batch_size=4,    # å¯ä»¥è¯•è¯• 4 æˆ– 8ï¼Œè®­ç»ƒæ›´ç¨³å®š
    save_steps=100,
    logging_steps=10,
    learning_rate=5e-5,               # è®¾ç½®å­¦ä¹ ç‡ï¼Œé»˜è®¤å¤ªä½å­¦ä¹ æ…¢
    fp16=False                        # å¦‚æœä½ ç”¨çš„æ˜¯ Mac Intel/ARM CPU
)



# âœ… æ•°æ®æ‹¼æ¥æ–¹å¼ï¼ˆç”¨äº Causal LMï¼‰
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# âœ… å¼€å§‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

print("ğŸš€ Starting training...")
trainer.train()
print("âœ… Training complete. Model saved to ./chat-model")
